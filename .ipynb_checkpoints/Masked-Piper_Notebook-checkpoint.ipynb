{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e71223b1",
   "metadata": {},
   "source": [
    "# Masked-Piper\n",
    "\n",
    "This python notebook runs you through the procedure of taking videos as inputs with a single person in the video, and outputting the 1) a masked video with facial, hand, and arm kinematics ovelayen, and 2) outputs the kinematic timeseries. This tool is a simple but effective modification of the the Holistic Tracking by Google's Mediapipe so that we can use it as a CPU-based light weigth tool to mask your video data while maintaining background information, and also preserving information about body kinematics. \n",
    "\n",
    "Current Github: https://github.com/WimPouw/TowardsMultimodalOpenScience\n",
    "\n",
    "## Additional information backbone of the tool (Mediapipe Holistic Tracking)\n",
    "https://google.github.io/mediapipe/solutions/holistic.html\n",
    "\n",
    "## Modification that is the basis of this tool\n",
    "Our modification of the Mediapipe tool is using the body sillhoette to distinguish the background from the body contained in the video, then track the body, and create new video that only keeps the background, masks the body, and overlays the kinematics back onto the mask. We further modify the original code so that timeseries are produced that provide all the kinematic information per frame over time.\n",
    "\n",
    "## Use\n",
    "Make sure to install all the packages in requirements.txt. Then move your videos that you want to mask into the input folder. Then run this code, which will loop through all the videos contained in the input folder; and saves all the results in the output folders.\n",
    "\n",
    "Please use, improve and adapt as you see fit. This tool will become citable in the near future.\n",
    "\n",
    "\n",
    "Team: Babajide Owoyele, James Trujillo, Wim Pouw (wim.pouw@donders.ru.nl)\n",
    "\n",
    "![Example](./Images/Capture.jpg)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "408e4958",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following video will be processed for masking: \n",
      "['sample.mp4', 'ted_kid.mp4']\n"
     ]
    }
   ],
   "source": [
    "#load in required packages\n",
    "import mediapipe as mp #mediapipe\n",
    "import cv2 #opencv\n",
    "import math #basic operations\n",
    "import numpy as np #basic operations\n",
    "import pandas as pd #data wrangling\n",
    "import csv #csv saving\n",
    "\n",
    "#list all videos in input_videofolder\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "mypath = \"./Input_Videos/\" #this is your folder with (all) your video(s)\n",
    "vfiles = [f for f in listdir(mypath) if isfile(join(mypath, f))] #loop through the filenames and collect them in a list\n",
    "#time series output folder\n",
    "outputf_mask = \"./Output_MaskedVideos/\"\n",
    "outtputf_ts = \"./Output_TimeSeries/\"\n",
    "\n",
    "#check videos to be processed\n",
    "print(\"The following video will be processed for masking: \")\n",
    "print(vfiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "b805ffb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize modules and functions\n",
    "\n",
    "#load in mediapipe modules\n",
    "mp_holistic = mp.solutions.holistic\n",
    "# Import drawing_utils and drawing_styles.\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_drawing_styles = mp.solutions.drawing_styles\n",
    "\n",
    "##################FUNCTIONS AND OTHER VARIABLES\n",
    "#landmarks 33x that are used by Mediapipe (Blazepose)\n",
    "markersbody = ['NOSE', 'LEFT_EYE_INNER', 'LEFT_EYE', 'LEFT_EYE_OUTER', 'RIGHT_EYE_OUTER', 'RIGHT_EYE', 'RIGHT_EYE_OUTER',\n",
    "          'LEFT_EAR', 'RIGHT_EAR', 'MOUTH_LEFT', 'MOUTH_RIGHT', 'LEFT_SHOULDER', 'RIGHT_SHOULDER', 'LEFT_ELBOW', \n",
    "          'RIGHT_ELBOW', 'LEFT_WRIST', 'RIGHT_WRIST', 'LEFT_PINKY', 'RIGHT_PINKY', 'LEFT_INDEX', 'RIGHT_INDEX',\n",
    "          'LEFT_THUMB', 'RIGHT_THUMB', 'LEFT_HIP', 'RIGHT_HIP', 'LEFT_KNEE', 'RIGHT_KNEE', 'LEFT_ANKLE', 'RIGHT_ANKLE',\n",
    "          'LEFT_HEEL', 'RIGHT_HEEL', 'LEFT_FOOT_INDEX', 'RIGHT_FOOT_INDEX']\n",
    "\n",
    "markershands = ['LEFT_WRIST', 'LEFT_THUMB_CMC', 'LEFT_THUMB_MCP', 'LEFT_THUMB_IP', 'LEFT_THUMB_TIP', 'LEFT_INDEX_FINGER_MCP',\n",
    "              'LEFT_INDEX_FINGER_PIP', 'LEFT_INDEX_FINGER_DIP', 'LEFT_INDEX_FINGER_TIP', 'LEFT_MIDDLE_FINGER_MCP', \n",
    "               'LEFT_MIDDLE_FINGER_PIP', 'LEFT_MIDDLE_FINGER_DIP', 'LEFT_MIDDLE_FINGER_TIP', 'LEFT_RING_FINGER_MCP', \n",
    "               'LEFT_RING_FINGER_PIP', 'LEFT_RING_FINGER_DIP', 'LEFT_RING_FINGER_TIP', 'LEFT_PINKY_FINGER_MCP', \n",
    "               'LEFT_PINKY_FINGER_PIP', 'LEFT_PINKY_FINGER_DIP', 'LEFT_PINKY_FINGER_TIP',\n",
    "              'RIGHT_WRIST', 'RIGHT_THUMB_CMC', 'RIGHT_THUMB_MCP', 'RIGHT_THUMB_IP', 'RIGHT_THUMB_TIP', 'RIGHT_INDEX_FINGER_MCP',\n",
    "              'RIGHT_INDEX_FINGER_PIP', 'RIGHT_INDEX_FINGER_DIP', 'RIGHT_INDEX_FINGER_TIP', 'RIGHT_MIDDLE_FINGER_MCP', \n",
    "               'RIGHT_MIDDLE_FINGER_PIP', 'RIGHT_MIDDLE_FINGER_DIP', 'RIGHT_MIDDLE_FINGER_TIP', 'RIGHT_RING_FINGER_MCP', \n",
    "               'RIGHT_RING_FINGER_PIP', 'RIGHT_RING_FINGER_DIP', 'RIGHT_RING_FINGER_TIP', 'RIGHT_PINKY_FINGER_MCP', \n",
    "               'RIGHT_PINKY_FINGER_PIP', 'RIGHT_PINKY_FINGER_DIP', 'RIGHT_PINKY_FINGER_TIP']\n",
    "facemarks = [str(x) for x in range(478)] #there are 478 points for the face mesh (see google holistic face mesh info for landmarks)\n",
    "\n",
    "\n",
    "#set up the column names and objects for the time series data (add time as the first variable)\n",
    "markerxyzbody = ['time']\n",
    "markerxyzhands = ['time']\n",
    "markerxyzface = ['time']\n",
    "\n",
    "for mark in markersbody:\n",
    "    for pos in ['X', 'Y', 'Z', 'visibility']: #for markers of the body you also have a visibility reliability score\n",
    "        nm = pos + \"_\" + mark\n",
    "        markerxyzbody.append(nm)\n",
    "for mark in markershands:\n",
    "    for pos in ['X', 'Y', 'Z']:\n",
    "        nm = pos + \"_\" + mark\n",
    "        markerxyzhands.append(nm)\n",
    "for mark in facemarks:\n",
    "    for pos in ['X', 'Y', 'Z']:\n",
    "        nm = pos + \"_\" + mark\n",
    "        markerxyzface.append(nm)\n",
    "\n",
    "#check if there are numbers in a string\n",
    "def num_there(s):\n",
    "    return any(i.isdigit() for i in s)\n",
    "\n",
    "#take some google classification object and convert it into a string\n",
    "def makegoginto_str(gogobj):\n",
    "    gogobj = str(gogobj).strip(\"[]\")\n",
    "    gogobj = gogobj.split(\"\\n\")\n",
    "    return(gogobj[:-1]) #ignore last element as this has nothing\n",
    "\n",
    "#make the stringifyd position traces into clean numerical values\n",
    "def listpostions(newsamplemarks):\n",
    "    newsamplemarks = makegoginto_str(newsamplemarks)\n",
    "    tracking_p = []\n",
    "    for value in newsamplemarks:\n",
    "        if num_there(value):\n",
    "            stripped = value.split(':', 1)[1]\n",
    "            stripped = stripped.strip() #remove spaces in the string if present\n",
    "            tracking_p.append(stripped) #add to this list  \n",
    "    return(tracking_p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a3f5b73",
   "metadata": {},
   "source": [
    "## Main procedure Masked-Piper\n",
    "The following chunk of code loops through all the videos you have loaded into the input folder, then assess each frame for body poses, extract kinematic info, masks the body in a new frame that keeps the background, projects the kinematic info on the mask, and stores the kinematic info for that frame into the time series .csv for the hand + body + face."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "5c9abb78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with processing all folders; go look in your output folders!\n"
     ]
    }
   ],
   "source": [
    "#make a loop\n",
    "for vidf in vfiles:\n",
    "    #capture the video, and check video settings\n",
    "    videoname = vidf\n",
    "    videoloc = \"./Input_Videos/\" + videoname\n",
    "    capture = cv2.VideoCapture(videoloc) #load in the videocapture\n",
    "    frameWidth = capture.get(cv2.CAP_PROP_FRAME_WIDTH) #check frame width\n",
    "    frameHeight = capture.get(cv2.CAP_PROP_FRAME_HEIGHT) #check frame height\n",
    "    samplerate = capture.get(cv2.CAP_PROP_FPS)   #fps = frames per second\n",
    "\n",
    "    #make an 'empty' video file where we project the pose tracking on\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'MP4V') #for different video formats you could use e.g., *'XVID'\n",
    "    out = cv2.VideoWriter('./Output_MaskedVideos/'+videoname, fourcc, \n",
    "                          fps = samplerate, frameSize = (int(frameWidth), int(frameHeight)))\n",
    "\n",
    "    # Run MediaPipe frame by frame using Holistic with `enable_segmentation=True` to get pose segmentation.\n",
    "    time = 0\n",
    "    tsbody = [markerxyzbody]   #these will be your time series objects, which start with collumn names initialized above\n",
    "    tshands = [markerxyzhands] #these will be your time series objects, which start with collumn names initialized above\n",
    "    tsface = [markerxyzface]   #these will be your time series objects, which start with collumn names initialized above\n",
    "    with mp_holistic.Holistic(\n",
    "            static_image_mode=True, enable_segmentation=True, refine_face_landmarks=True) as holistic:\n",
    "        while (True):\n",
    "            ret, image = capture.read() #read frame\n",
    "            if ret == True: #if there is a frame\n",
    "                image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) #make sure the image is in RGB format\n",
    "                results = holistic.process(image) #apply Mediapipe holistic processing\n",
    "                # Draw pose segmentation\n",
    "                h, w, c = image.shape\n",
    "                original_image = np.concatenate([image, np.full((h, w, 1), 255, dtype=np.uint8)], axis=-1)\n",
    "                mask_img = np.zeros_like(image, dtype=np.uint8) #set up basic mask image\n",
    "                mask_img[:, :] = (255,255,255) #set up basic mask image\n",
    "                segm_2class = 0.2 + 0.8 * results.segmentation_mask #set up a segmentation of the results of mediapipe\n",
    "                segm_2class = np.repeat(segm_2class[..., np.newaxis], 3, axis=2) #set up a segmentation of the results of mediapipe\n",
    "                annotated_image = mask_img * segm_2class * (1 - segm_2class) #take the basic mask image and make a sillhouette mask\n",
    "                # append Alpha channel to sillhouetted mask so that we can overlay it to the original image\n",
    "                mask = np.concatenate([annotated_image, np.full((h, w, 1), 255, dtype=np.uint8)], axis=-1)\n",
    "                # Zero background where we want to overlay\n",
    "                original_image[mask==0]=0 #for the original image we are going to set everything at zero for places where the mask has to go\n",
    "                original_image = cv2.cvtColor(original_image, cv2.COLOR_RGB2BGR)\n",
    "                #now lets draw on the original_image the left and right hand landmarks, the facemesh and the body poses\n",
    "                    #left hand\n",
    "                mp_drawing.draw_landmarks(original_image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS)\n",
    "                    #right hand\n",
    "                mp_drawing.draw_landmarks(original_image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS)\n",
    "                    #face\n",
    "                mp_drawing.draw_landmarks(\n",
    "                        original_image,\n",
    "                        results.face_landmarks,\n",
    "                        mp_holistic.FACEMESH_TESSELATION,\n",
    "                        landmark_drawing_spec=None,\n",
    "                        connection_drawing_spec=mp_drawing_styles\n",
    "                        .get_default_face_mesh_tesselation_style())\n",
    "                    #body\n",
    "                mp_drawing.draw_landmarks(\n",
    "                        original_image,\n",
    "                        results.pose_landmarks,\n",
    "                        mp_holistic.POSE_CONNECTIONS,\n",
    "                        landmark_drawing_spec=mp_drawing_styles.\n",
    "                        get_default_pose_landmarks_style())\n",
    "                #######################now save everything to a time series\n",
    "                    #make a variable list with x, y, z, info where data is appended to\n",
    "                samplebody = listpostions(results.pose_landmarks)\n",
    "                samplehands = listpostions([results.left_hand_landmarks, results.right_hand_landmarks])\n",
    "                sampleface = listpostions(results.face_landmarks)\n",
    "                samplebody.insert(0, time)\n",
    "                samplehands.insert(0, time)\n",
    "                sampleface.insert(0, time)\n",
    "                tsbody.append(samplebody)   #append to the timeseries object\n",
    "                tshands.append(samplehands) #append to the timeseries object\n",
    "                tsface.append(sampleface)   #append to the timeseries object\n",
    "                #show the video as we process (you can comment this out, if you want to run this process in the background)\n",
    "                cv2.imshow(\"resizedimage\", original_image)\n",
    "                out.write(original_image) #save the frame to the new masked video\n",
    "                time = time+(1000/samplerate)#update the time variable  for the next frame\n",
    "            if cv2.waitKey(1) == 27: #allow the use of ESCAPE to break the loop\n",
    "                   break\n",
    "            if ret == False: #if there are no more frames, break the loop\n",
    "                break\n",
    "\n",
    "    #once done de-initialize all processes\n",
    "    out.release()\n",
    "    capture.release()\n",
    "    cv2.destroyAllWindows()\n",
    "     ####################################################### data to be written row-wise in csv fil\n",
    "    # opening the csv file in 'w+' mode\n",
    "    filebody = open(\"./Output_TimeSeries/\" + vidf[:-4]+'_body.csv', 'w+', newline ='')\n",
    "    #write it\n",
    "    with filebody:    \n",
    "        write = csv.writer(filebody)\n",
    "        write.writerows(tsbody)\n",
    "     # opening the csv file in 'w+' mode\n",
    "    filehands = open(\"./Output_TimeSeries/\" + vidf[:-4]+'_hands.csv', 'w+', newline ='')\n",
    "    #write it\n",
    "    with filehands:\n",
    "        write = csv.writer(filehands)\n",
    "        write.writerows(tshands)\n",
    "    # opening the csv file in 'w+' mode\n",
    "    fileface = open(\"./Output_TimeSeries/\" + vidf[:-4]+'_face.csv', 'w+', newline ='')\n",
    "    #write it\n",
    "    with fileface:    \n",
    "        write = csv.writer(fileface)\n",
    "        write.writerows(tsface)\n",
    "\n",
    "print(\"Done with processing all folders; go look in your output folders!\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5096e4f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f52926b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
